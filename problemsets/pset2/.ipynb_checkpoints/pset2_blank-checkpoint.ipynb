{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2: Merging and regular expressions\n",
    "\n",
    "**Total points (without extra credit)**: 30 \n",
    "\n",
    "**Background on the policy context**: here, we're going to use two datasets to practice reshaping, merging, and regular expression patterns. Both datasets relate to the broader issue of which employers might be violating the rights of temporary guestworkers granted visas under the H-2A program. Here are some articles about potential exploitation of guestworkers by firms and inequality caused by minimal oversight:\n",
    "\n",
    "- News media coverage of labor abuses of temporary guestworkers: https://www.buzzfeednews.com/article/kenbensinger/the-pushovers \n",
    "- GAO report on labor abuses of temporary guestworkers: https://www.gao.gov/products/gao-15-154\n",
    "\n",
    "The following datasets are located in `pset2_inputdata` (need to unzip): \n",
    "\n",
    "- `jobs_clean`: a dataset of guestworker jobs posted by many employers, some of whom have been debarred (banned) from the program for labor abuses; others not debarred\n",
    "- `debar`: a dataset of employers who committed violations of labor regulations meant to protect temporary guestworkers \n",
    "\n",
    "\n",
    "You can view a codebook here: https://docs.google.com/spreadsheets/d/1rF9GJEC8pPKxipD0TsoG9DVdqz3EJ-b-BHEtyioAX7I/edit?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "## repeated printouts\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reshaping data (13 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the following dataset stored in `pset2_inputdata`: `debar.csv`\n",
    "\n",
    "This represents employers temporarily banned from hiring workers (debar.csv); call this `debar`\n",
    "\n",
    "\n",
    "View the head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J&amp;J Harvesting</td>\n",
       "      <td>Leads, ND</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>1/19/2014</td>\n",
       "      <td>1/18/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stahlman Apiaries, Inc</td>\n",
       "      <td>Selby, SD</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>1 year</td>\n",
       "      <td>2/19/2015</td>\n",
       "      <td>2/14/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trust Nursery</td>\n",
       "      <td>Pulaski, NY</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>1 year</td>\n",
       "      <td>3/21/2014</td>\n",
       "      <td>3/20/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anton Fertilizer Inc.</td>\n",
       "      <td>Dighton, KS</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>3/30/2014</td>\n",
       "      <td>3/29/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great Plains Fluid Service, Inc.</td>\n",
       "      <td>Greensburg, KS</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>3/30/2014</td>\n",
       "      <td>3/29/2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Name     City, State  \\\n",
       "0                    J&J Harvesting       Leads, ND   \n",
       "1            Stahlman Apiaries, Inc       Selby, SD   \n",
       "2                     Trust Nursery     Pulaski, NY   \n",
       "3             Anton Fertilizer Inc.     Dighton, KS   \n",
       "4  Great Plains Fluid Service, Inc.  Greensburg, KS   \n",
       "\n",
       "                                        Violation Duration Start date  \\\n",
       "0  Failure to respond to audit (partial response)  2 years  1/19/2014   \n",
       "1  Failure to respond to audit (partial response)   1 year  2/19/2015   \n",
       "2  Failure to respond to audit (partial response)   1 year  3/21/2014   \n",
       "3       Failure to respond to audit (no response)  2 years  3/30/2014   \n",
       "4       Failure to respond to audit (no response)  2 years  3/30/2014   \n",
       "\n",
       "    End date  \n",
       "0  1/18/2016  \n",
       "1  2/14/2016  \n",
       "2  3/20/2015  \n",
       "3  3/29/2016  \n",
       "4  3/29/2016  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "debar = pd.read_csv('./pset2_inputdata/debar.csv')\n",
    "\n",
    "debar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 (1 point)\n",
    "\n",
    "Print the number of rows in `debar` versus the number of unique employer names (`Name`). Is there one row per employer or multiple rows for some employers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debar row_count: 114\n",
      "unique employer names: 98\n",
      "Since there are more rows than number of unique employers, there are multiple rows for some employers\n"
     ]
    }
   ],
   "source": [
    "## your code here\n",
    "row_count = len(debar)\n",
    "print(\"debar row_count:\",row_count)\n",
    "num_unique = debar['Name'].nunique()\n",
    "print(\"unique employer names:\",num_unique)\n",
    "\n",
    "print(\"Since there are more rows than number of unique employers, there are multiple rows for some employers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Investigating duplicated rows (2 points)\n",
    "\n",
    "A. Create a new column in `debar`--`is_repeated`-- that tells us whether an employer (`Name`) is repeated > 1 times\n",
    "\n",
    "*Hint*: there are multiple ways to solve this but some possibilities to get the list of names that are repeated are:\n",
    "- Using value_counts() on the `Name` variable and extracting the index from that value counts \n",
    "- Using groupby to count the rows attached to one name\n",
    "\n",
    "B. Print the rows where `is_repeated == True` and interpret\n",
    "\n",
    "C. Subset to the rows where `is_repeated == True` and save that data as `mult_debar`. Print the head() and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Annabella Land &amp; Cattle</td>\n",
       "      <td>Annabella, UT</td>\n",
       "      <td>Non Payment</td>\n",
       "      <td>1 year</td>\n",
       "      <td>5/9/2014</td>\n",
       "      <td>5/9/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autumn Hill Orchard</td>\n",
       "      <td>Groton, MA</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/5/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Caddo Creek Ranch, dba Paradise Ranch</td>\n",
       "      <td>Caddo, TX</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/20/2014</td>\n",
       "      <td>7/19/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Loewen Harvesting LLC</td>\n",
       "      <td>Brownsville, TX</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>1 year</td>\n",
       "      <td>8/20/2014</td>\n",
       "      <td>8/19/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rollo Farm Labor Contractor</td>\n",
       "      <td>Miami, FL</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>8/23/2014</td>\n",
       "      <td>8/22/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sharon Mathis</td>\n",
       "      <td>Tifton, GA</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SRT Farms</td>\n",
       "      <td>Morton, TX</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mark Duncan</td>\n",
       "      <td>Roosevelt, UT</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Maple Ridge Custom Services, LLC</td>\n",
       "      <td>Altheimer, AK</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>F&amp;W Farms</td>\n",
       "      <td>Ingalls, KS</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>12/10/2014</td>\n",
       "      <td>12/9/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cisco Produce Inc.</td>\n",
       "      <td>Cairo, GA</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>12/10/2014</td>\n",
       "      <td>12/9/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Old Tree Farms/Verpaalen Custom Service</td>\n",
       "      <td>Volga, SD</td>\n",
       "      <td>WHD Debarment</td>\n",
       "      <td>3 years</td>\n",
       "      <td>12/11/2014</td>\n",
       "      <td>12/10/2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Rollo Farm Labor Contractor</td>\n",
       "      <td>Miami, FL</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>8/23/2014</td>\n",
       "      <td>8/22/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Loewen Harvesting LLC</td>\n",
       "      <td>Brownfield, TX</td>\n",
       "      <td>Impeding the Audit Process – Partial- Response</td>\n",
       "      <td>1 year</td>\n",
       "      <td>8/20/2014</td>\n",
       "      <td>8/19/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Caddo Creek Ranch, dba Paradise Ranch</td>\n",
       "      <td>Caddo, Texas</td>\n",
       "      <td>Impeding the Audit Process – Partial- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/20/2014</td>\n",
       "      <td>7/19/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Autumn Hill Orchard</td>\n",
       "      <td>Groton, MA</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/5/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Annabella Land &amp; Cattle</td>\n",
       "      <td>Annabella, Utah</td>\n",
       "      <td>Non-payment</td>\n",
       "      <td>1 year</td>\n",
       "      <td>5/9/2014</td>\n",
       "      <td>5/8/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Macky and Brad Farms</td>\n",
       "      <td>Plains, TX</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>1 year</td>\n",
       "      <td>2/13/2015</td>\n",
       "      <td>2/12/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Old Tree Farms/Verpaalen Custom Service</td>\n",
       "      <td>Volga, SD</td>\n",
       "      <td>Wage Hour Debarment</td>\n",
       "      <td>3 years</td>\n",
       "      <td>12/1/2014</td>\n",
       "      <td>12/1/2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Macky and Brad Farms</td>\n",
       "      <td>Plains, TX</td>\n",
       "      <td>Impeding the Audit Process – Partial- Response</td>\n",
       "      <td>1 year</td>\n",
       "      <td>2/13/2015</td>\n",
       "      <td>2/12/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Cisco Produce Inc.</td>\n",
       "      <td>Cairo, GA</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>12/10/2015</td>\n",
       "      <td>12/9/2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>F&amp;W Farms</td>\n",
       "      <td>Ingalls, KS</td>\n",
       "      <td>Impeding the Audit Process – Partial- Response</td>\n",
       "      <td>1 year</td>\n",
       "      <td>12/10/2014</td>\n",
       "      <td>12/9/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Maple Ridge Custom Services, LLC</td>\n",
       "      <td>Altheimer, AR</td>\n",
       "      <td>Impeding the Audit Process – Partial- Response</td>\n",
       "      <td>1 year</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Mark Duncan</td>\n",
       "      <td>Roosevelt, UT</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SRT Farms</td>\n",
       "      <td>Morton, TX</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Sharon Mathis</td>\n",
       "      <td>Tifton, GA</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>11/16/2014</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Xavier Horne</td>\n",
       "      <td>Lyons, Georgia</td>\n",
       "      <td>Non-payment of certification fee</td>\n",
       "      <td>1 year</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>6/15/2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Xavier Horne</td>\n",
       "      <td>Lyons, Georgia</td>\n",
       "      <td>Failure to respond to audit request</td>\n",
       "      <td>2 years</td>\n",
       "      <td>9/27/2017</td>\n",
       "      <td>9/26/2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Dove Creek Farms</td>\n",
       "      <td>Mount Vernon, TX</td>\n",
       "      <td>Failure to respond to audit request</td>\n",
       "      <td>2 years</td>\n",
       "      <td>2/9/2018</td>\n",
       "      <td>2/8/2018</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Turner Farms</td>\n",
       "      <td>Healy, KS</td>\n",
       "      <td>Failure to comply with the employer's obligati...</td>\n",
       "      <td>6 months</td>\n",
       "      <td>7/17/19</td>\n",
       "      <td>2/10/2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Dove Creek Farms</td>\n",
       "      <td>Mount Vernon, TX</td>\n",
       "      <td>Failure to Respond to Audit Request</td>\n",
       "      <td>2 years</td>\n",
       "      <td>2/9/2018</td>\n",
       "      <td>2/8/2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Turner Farms</td>\n",
       "      <td>Healy, KS</td>\n",
       "      <td>Failure to comply with the employer's obligati...</td>\n",
       "      <td>7 months</td>\n",
       "      <td>7/17/19</td>\n",
       "      <td>2/10/20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name       City, State  \\\n",
       "6                    Annabella Land & Cattle     Annabella, UT   \n",
       "7                        Autumn Hill Orchard        Groton, MA   \n",
       "8      Caddo Creek Ranch, dba Paradise Ranch         Caddo, TX   \n",
       "11                     Loewen Harvesting LLC   Brownsville, TX   \n",
       "12               Rollo Farm Labor Contractor         Miami, FL   \n",
       "14                             Sharon Mathis        Tifton, GA   \n",
       "15                                 SRT Farms        Morton, TX   \n",
       "16                               Mark Duncan     Roosevelt, UT   \n",
       "17          Maple Ridge Custom Services, LLC     Altheimer, AK   \n",
       "18                                 F&W Farms       Ingalls, KS   \n",
       "19                        Cisco Produce Inc.         Cairo, GA   \n",
       "21   Old Tree Farms/Verpaalen Custom Service         Volga, SD   \n",
       "24               Rollo Farm Labor Contractor         Miami, FL   \n",
       "25                     Loewen Harvesting LLC    Brownfield, TX   \n",
       "28     Caddo Creek Ranch, dba Paradise Ranch      Caddo, Texas   \n",
       "29                       Autumn Hill Orchard        Groton, MA   \n",
       "30                   Annabella Land & Cattle   Annabella, Utah   \n",
       "31                      Macky and Brad Farms        Plains, TX   \n",
       "51   Old Tree Farms/Verpaalen Custom Service         Volga, SD   \n",
       "55                      Macky and Brad Farms        Plains, TX   \n",
       "56                        Cisco Produce Inc.         Cairo, GA   \n",
       "58                                 F&W Farms       Ingalls, KS   \n",
       "59          Maple Ridge Custom Services, LLC     Altheimer, AR   \n",
       "60                               Mark Duncan     Roosevelt, UT   \n",
       "61                                 SRT Farms        Morton, TX   \n",
       "62                             Sharon Mathis        Tifton, GA   \n",
       "73                              Xavier Horne    Lyons, Georgia   \n",
       "89                              Xavier Horne    Lyons, Georgia   \n",
       "103                         Dove Creek Farms  Mount Vernon, TX   \n",
       "106                             Turner Farms         Healy, KS   \n",
       "109                         Dove Creek Farms  Mount Vernon, TX   \n",
       "111                             Turner Farms         Healy, KS   \n",
       "\n",
       "                                             Violation  Duration  Start date  \\\n",
       "6                                          Non Payment    1 year    5/9/2014   \n",
       "7            Failure to respond to audit (no response)   2 years    7/6/2014   \n",
       "8       Failure to respond to audit (partial response)   2 years   7/20/2014   \n",
       "11      Failure to respond to audit (partial response)    1 year   8/20/2014   \n",
       "12           Failure to respond to audit (no response)   2 years   8/23/2014   \n",
       "14           Failure to respond to audit (no response)   2 years  11/16/2014   \n",
       "15           Failure to respond to audit (no response)   2 years  11/16/2014   \n",
       "16           Failure to respond to audit (no response)   2 years  11/16/2014   \n",
       "17      Failure to respond to audit (partial response)   2 years  11/16/2014   \n",
       "18      Failure to respond to audit (partial response)   2 years  12/10/2014   \n",
       "19           Failure to respond to audit (no response)   2 years  12/10/2014   \n",
       "21                                       WHD Debarment   3 years  12/11/2014   \n",
       "24          Impeding the Audit Process – Non- Response   2 years   8/23/2014   \n",
       "25      Impeding the Audit Process – Partial- Response    1 year   8/20/2014   \n",
       "28      Impeding the Audit Process – Partial- Response   2 years   7/20/2014   \n",
       "29          Impeding the Audit Process – Non- Response   2 years    7/6/2014   \n",
       "30                                         Non-payment    1 year    5/9/2014   \n",
       "31           Failure to respond to audit (no response)    1 year   2/13/2015   \n",
       "51                                 Wage Hour Debarment   3 years   12/1/2014   \n",
       "55      Impeding the Audit Process – Partial- Response    1 year   2/13/2015   \n",
       "56          Impeding the Audit Process – Non- Response   2 years  12/10/2015   \n",
       "58      Impeding the Audit Process – Partial- Response    1 year  12/10/2014   \n",
       "59      Impeding the Audit Process – Partial- Response    1 year  11/16/2014   \n",
       "60          Impeding the Audit Process – Non- Response   2 years  11/16/2014   \n",
       "61          Impeding the Audit Process – Non- Response   2 years  11/16/2014   \n",
       "62          Impeding the Audit Process – Non- Response   2 years  11/16/2014   \n",
       "73                    Non-payment of certification fee    1 year   6/16/2016   \n",
       "89                 Failure to respond to audit request   2 years   9/27/2017   \n",
       "103                Failure to respond to audit request   2 years    2/9/2018   \n",
       "106  Failure to comply with the employer's obligati...  6 months     7/17/19   \n",
       "109                Failure to Respond to Audit Request   2 years    2/9/2018   \n",
       "111  Failure to comply with the employer's obligati...  7 months     7/17/19   \n",
       "\n",
       "       End date  is_repeated  \n",
       "6      5/9/2015         True  \n",
       "7      7/5/2016         True  \n",
       "8     7/19/2016         True  \n",
       "11    8/19/2015         True  \n",
       "12    8/22/2016         True  \n",
       "14   11/15/2016         True  \n",
       "15   11/15/2016         True  \n",
       "16   11/15/2016         True  \n",
       "17   11/15/2016         True  \n",
       "18    12/9/2016         True  \n",
       "19    12/9/2016         True  \n",
       "21   12/10/2017         True  \n",
       "24    8/22/2016         True  \n",
       "25    8/19/2015         True  \n",
       "28    7/19/2016         True  \n",
       "29     7/5/2016         True  \n",
       "30     5/8/2015         True  \n",
       "31    2/12/2016         True  \n",
       "51    12/1/2017         True  \n",
       "55    2/12/2016         True  \n",
       "56    12/9/2017         True  \n",
       "58    12/9/2015         True  \n",
       "59   11/15/2015         True  \n",
       "60   11/15/2016         True  \n",
       "61   11/15/2016         True  \n",
       "62   11/15/2016         True  \n",
       "73    6/15/2017         True  \n",
       "89    9/26/2019         True  \n",
       "103    2/8/2018         True  \n",
       "106   2/10/2020         True  \n",
       "109    2/8/2020         True  \n",
       "111     2/10/20         True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Annabella Land &amp; Cattle</td>\n",
       "      <td>Annabella, UT</td>\n",
       "      <td>Non Payment</td>\n",
       "      <td>1 year</td>\n",
       "      <td>5/9/2014</td>\n",
       "      <td>5/9/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autumn Hill Orchard</td>\n",
       "      <td>Groton, MA</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/5/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Caddo Creek Ranch, dba Paradise Ranch</td>\n",
       "      <td>Caddo, TX</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/20/2014</td>\n",
       "      <td>7/19/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Loewen Harvesting LLC</td>\n",
       "      <td>Brownsville, TX</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>1 year</td>\n",
       "      <td>8/20/2014</td>\n",
       "      <td>8/19/2015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rollo Farm Labor Contractor</td>\n",
       "      <td>Miami, FL</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>8/23/2014</td>\n",
       "      <td>8/22/2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Name      City, State  \\\n",
       "6                 Annabella Land & Cattle    Annabella, UT   \n",
       "7                     Autumn Hill Orchard       Groton, MA   \n",
       "8   Caddo Creek Ranch, dba Paradise Ranch        Caddo, TX   \n",
       "11                  Loewen Harvesting LLC  Brownsville, TX   \n",
       "12            Rollo Farm Labor Contractor        Miami, FL   \n",
       "\n",
       "                                         Violation Duration Start date  \\\n",
       "6                                      Non Payment   1 year   5/9/2014   \n",
       "7        Failure to respond to audit (no response)  2 years   7/6/2014   \n",
       "8   Failure to respond to audit (partial response)  2 years  7/20/2014   \n",
       "11  Failure to respond to audit (partial response)   1 year  8/20/2014   \n",
       "12       Failure to respond to audit (no response)  2 years  8/23/2014   \n",
       "\n",
       "     End date  is_repeated  \n",
       "6    5/9/2015         True  \n",
       "7    7/5/2016         True  \n",
       "8   7/19/2016         True  \n",
       "11  8/19/2015         True  \n",
       "12  8/22/2016         True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(32, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "rname = debar.Name.value_counts()\n",
    "repeats = rname[rname > 1].index\n",
    "debar[\"is_repeated\"] = debar.Name.isin(repeats)\n",
    "\n",
    "debar[debar.is_repeated==True]\n",
    "#The majority of firms with repeated violations are related to auditing procedures and occurred between 2014-2015. \n",
    "\n",
    "mult_debar = debar[debar.is_repeated==True]\n",
    "mult_debar.head()\n",
    "mult_debar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Reshape mult_debar to wide to begin filtering out duplicates (4 points)\n",
    "\n",
    "You want to separate out two cases:\n",
    "\n",
    "- Cases where the repeat rows for one employer are due to duplicated data \n",
    "- Cases where the repeat rows for one employer represent repeated violations for different issues\n",
    "\n",
    "There are various ways to check duplicates in this data (eg converting `Violation` to lowercase; replacing spelled-out states with two-dig state codes)\n",
    "\n",
    "We're going to use the simple rule of:\n",
    "\n",
    "- A row is a duplicate if, within an employer (defined by Name + City, State), the Start date for each row's violation is the same \n",
    "\n",
    "To begin to check this, reshape `mult_debar` to a wide dataframe (`mult_debar_wide`) with the following columns, treating the `Name` and `City, State` as the index for the pivot:\n",
    "\n",
    "- Name\n",
    "- City, State\n",
    "- start_date_viol1\n",
    "- start_date_viol2\n",
    "\n",
    "Print the head and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/1hb6xh85081_hpl5qx7hz9fr0000gn/T/ipykernel_58671/78598671.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mult_debar.loc[:, 'viol_num'] = mult_debar.groupby('Name').cumcount() + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City, State</th>\n",
       "      <th>start_date_viol1</th>\n",
       "      <th>start_date_viol2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Annabella Land &amp; Cattle</th>\n",
       "      <td>Annabella, UT</td>\n",
       "      <td>5/9/2014</td>\n",
       "      <td>5/9/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autumn Hill Orchard</th>\n",
       "      <td>Groton, MA</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/6/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caddo Creek Ranch, dba Paradise Ranch</th>\n",
       "      <td>Caddo, TX</td>\n",
       "      <td>7/20/2014</td>\n",
       "      <td>7/20/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cisco Produce Inc.</th>\n",
       "      <td>Cairo, GA</td>\n",
       "      <td>12/10/2014</td>\n",
       "      <td>12/10/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dove Creek Farms</th>\n",
       "      <td>Mount Vernon, TX</td>\n",
       "      <td>2/9/2018</td>\n",
       "      <td>2/9/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            City, State start_date_viol1  \\\n",
       "Name                                                                       \n",
       "Annabella Land & Cattle                   Annabella, UT         5/9/2014   \n",
       "Autumn Hill Orchard                          Groton, MA         7/6/2014   \n",
       "Caddo Creek Ranch, dba Paradise Ranch         Caddo, TX        7/20/2014   \n",
       "Cisco Produce Inc.                            Cairo, GA       12/10/2014   \n",
       "Dove Creek Farms                       Mount Vernon, TX         2/9/2018   \n",
       "\n",
       "                                      start_date_viol2  \n",
       "Name                                                    \n",
       "Annabella Land & Cattle                       5/9/2014  \n",
       "Autumn Hill Orchard                           7/6/2014  \n",
       "Caddo Creek Ranch, dba Paradise Ranch        7/20/2014  \n",
       "Cisco Produce Inc.                          12/10/2015  \n",
       "Dove Creek Farms                              2/9/2018  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(16, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "mult_debar.loc[:, 'viol_num'] = mult_debar.groupby('Name').cumcount() + 1\n",
    "\n",
    "mult_debar_wide = mult_debar.pivot(index='Name', \n",
    "                                   columns='viol_num', \n",
    "                                   values='Start date')\n",
    "\n",
    "mult_debar_wide.columns = ['start_date_viol1', 'start_date_viol2']\n",
    "\n",
    "state_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
    "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
    "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
    "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "mult_debar.loc[:, \"City, State\"] = mult_debar[\"City, State\"].replace(state_abbrev)\n",
    "\n",
    "city_state_map = mult_debar.drop_duplicates('Name')[['Name', 'City, State']].set_index('Name')\n",
    "mult_debar_wide[\"City, State\"] = mult_debar_wide.index.map(city_state_map[\"City, State\"])\n",
    "\n",
    "cols = mult_debar_wide.columns.tolist()\n",
    "cols.remove(\"City, State\")\n",
    "cols.insert(0, \"City, State\")\n",
    "mult_debar_wide = mult_debar_wide[cols]\n",
    "\n",
    "mult_debar_wide.head()\n",
    "mult_debar_wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Filter out duplicates from original debar data (6 points)\n",
    "\n",
    "A. Using `mult_debar_wide`, add a column `is_dup` that takes value of True for cases where start_date_viol1 == start_date_viol2 marking the row as a duplicate\n",
    "\n",
    "B. Going back to the original long-format data you loaded at the beginning- `debar`\n",
    "    - For employers where `is_dup == True` as indicated by your wide-format dataframe, only keep `violnum == viol1`\n",
    "    - For all other employers (so is_dup == False and ones we didnt need to check duplicates for), keep all violnum\n",
    "    - Remove the `is_repeated` column from the `debar` data\n",
    "\n",
    "**Hint**: you can complete part B without a for loop; `pd.concat` with axis = 0 (row binding) is one way\n",
    "\n",
    "Call the resulting dataframe `debar_clean` and print the shape and # of unique employer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "      <th>is_dup</th>\n",
       "      <th>violnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Old Tree Farms/Verpaalen Custom Service</td>\n",
       "      <td>Volga, SD</td>\n",
       "      <td>Wage Hour Debarment</td>\n",
       "      <td>3 years</td>\n",
       "      <td>12/1/2014</td>\n",
       "      <td>12/1/2017</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Cisco Produce Inc.</td>\n",
       "      <td>Cairo, GA</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "      <td>2 years</td>\n",
       "      <td>12/10/2015</td>\n",
       "      <td>12/9/2017</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Xavier Horne</td>\n",
       "      <td>Lyons, Georgia</td>\n",
       "      <td>Failure to respond to audit request</td>\n",
       "      <td>2 years</td>\n",
       "      <td>9/27/2017</td>\n",
       "      <td>9/26/2019</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Name     City, State  \\\n",
       "51  Old Tree Farms/Verpaalen Custom Service       Volga, SD   \n",
       "56                       Cisco Produce Inc.       Cairo, GA   \n",
       "89                             Xavier Horne  Lyons, Georgia   \n",
       "\n",
       "                                     Violation Duration  Start date  \\\n",
       "51                         Wage Hour Debarment  3 years   12/1/2014   \n",
       "56  Impeding the Audit Process – Non- Response  2 years  12/10/2015   \n",
       "89         Failure to respond to audit request  2 years   9/27/2017   \n",
       "\n",
       "     End date  is_repeated is_dup  violnum  \n",
       "51  12/1/2017         True  False        2  \n",
       "56  12/9/2017         True  False        2  \n",
       "89  9/26/2019         True  False        2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "mult_debar_wide[\"is_dup\"] = mult_debar_wide.start_date_viol1 == mult_debar_wide.start_date_viol2\n",
    "\n",
    "debar_copy = debar.copy()\n",
    "\n",
    "debar_copy = debar_copy.merge(mult_debar_wide[['is_dup']], left_on='Name', right_index=True, how='left')\n",
    "debar_copy['violnum'] = debar_copy.groupby('Name').cumcount() + 1\n",
    "\n",
    "dup_filtered = debar_copy[(debar_copy['is_dup'] == True) & (debar_copy['violnum'] == 1)]\n",
    "nondup_filtered = debar_copy[debar_copy['is_dup'] != True]\n",
    "\n",
    "debar_clean = pd.concat([dup_filtered, nondup_filtered], axis=0)\n",
    "debar_clean[debar_clean.violnum>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Merging and regex (17 points total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load data on job postings\n",
    "\n",
    "The previous dataset contains a small subset of employers who faced temporary bans due to violations of H-2A program regulations\n",
    "\n",
    "Since most of the bans have expired, we're going to see which of those employers posted new H-2A jobs in the first quarter of 2021 \n",
    "\n",
    "Loading the `jobs_clean.csv` data stored in `pset2_inputdata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NUMBER</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>RECEIVED_DATE</th>\n",
       "      <th>DECISION_DATE</th>\n",
       "      <th>TYPE_OF_EMPLOYER_APPLICATION</th>\n",
       "      <th>H2A_LABOR_CONTRACTOR</th>\n",
       "      <th>NATURE_OF_TEMPORARY_NEED</th>\n",
       "      <th>EMERGENCY_FILING</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>TRADE_NAME_DBA</th>\n",
       "      <th>...</th>\n",
       "      <th>ADDENDUM_B_HOUSING_ATTACHED</th>\n",
       "      <th>TOTAL_HOUSING_RECORDS</th>\n",
       "      <th>MEALS_PROVIDED</th>\n",
       "      <th>MEALS_CHARGED</th>\n",
       "      <th>MEAL_REIMBURSEMENT_MINIMUM</th>\n",
       "      <th>MEAL_REIMBURSEMENT_MAXIMUM</th>\n",
       "      <th>PHONE_TO_APPLY</th>\n",
       "      <th>EMAIL_TO_APPLY</th>\n",
       "      <th>WEBSITE_TO_APPLY</th>\n",
       "      <th>TOTAL_ADDENDUM_A_RECORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-300-20199-721302</td>\n",
       "      <td>Determination Issued - Withdrawn</td>\n",
       "      <td>2020-07-17 14:50:40.840</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fazio Farms Operating Company, LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>12.68</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>13607017661</td>\n",
       "      <td>faziofarms@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-300-20231-773906</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-20 10:38:15.620</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Association - Agent</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>Charlie Sunderland</td>\n",
       "      <td>Panter &amp; Sunderland Nursery</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>19318083783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.jobs4tn.gov/vosnet/Default.aspx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-300-20231-774123</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-24 15:33:14.340</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>Michael Rudebusch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>19369333827</td>\n",
       "      <td>fayethlynpitre@rocketmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-300-20231-774151</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-21 12:08:09.760</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>Lodahl Farms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14069637560</td>\n",
       "      <td>lodahl_kelsey@yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-300-20231-774508</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-20 10:17:34.530</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>Y</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>Dunson Harvesting, Inc.</td>\n",
       "      <td>Dunson Harvesting, Inc.</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>8</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18632939888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.employflorida.com</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CASE_NUMBER                           CASE_STATUS  \\\n",
       "0  H-300-20199-721302      Determination Issued - Withdrawn   \n",
       "1  H-300-20231-773906  Determination Issued - Certification   \n",
       "2  H-300-20231-774123  Determination Issued - Certification   \n",
       "3  H-300-20231-774151  Determination Issued - Certification   \n",
       "4  H-300-20231-774508  Determination Issued - Certification   \n",
       "\n",
       "             RECEIVED_DATE            DECISION_DATE  \\\n",
       "0  2020-07-17 14:50:40.840  2020-10-01 00:00:00.000   \n",
       "1  2020-08-20 10:38:15.620  2020-10-01 00:00:00.000   \n",
       "2  2020-08-24 15:33:14.340  2020-10-01 00:00:00.000   \n",
       "3  2020-08-21 12:08:09.760  2020-10-01 00:00:00.000   \n",
       "4  2020-08-20 10:17:34.530  2020-10-01 00:00:00.000   \n",
       "\n",
       "  TYPE_OF_EMPLOYER_APPLICATION H2A_LABOR_CONTRACTOR NATURE_OF_TEMPORARY_NEED  \\\n",
       "0          Individual Employer                    N                 Seasonal   \n",
       "1          Association - Agent                    N                 Seasonal   \n",
       "2          Individual Employer                    N                 Seasonal   \n",
       "3          Individual Employer                    N                 Seasonal   \n",
       "4          Individual Employer                    Y                 Seasonal   \n",
       "\n",
       "  EMERGENCY_FILING                       EMPLOYER_NAME  \\\n",
       "0                Y  Fazio Farms Operating Company, LLC   \n",
       "1                N                  Charlie Sunderland   \n",
       "2                N                   Michael Rudebusch   \n",
       "3                N                        Lodahl Farms   \n",
       "4                N             Dunson Harvesting, Inc.   \n",
       "\n",
       "                TRADE_NAME_DBA  ... ADDENDUM_B_HOUSING_ATTACHED  \\\n",
       "0                          NaN  ...                           N   \n",
       "1  Panter & Sunderland Nursery  ...                           N   \n",
       "2                          NaN  ...                           N   \n",
       "3                          NaN  ...                           Y   \n",
       "4      Dunson Harvesting, Inc.  ...                           Y   \n",
       "\n",
       "  TOTAL_HOUSING_RECORDS MEALS_PROVIDED MEALS_CHARGED  \\\n",
       "0                     1              Y         12.68   \n",
       "1                     1              N           NaN   \n",
       "2                     1              N           NaN   \n",
       "3                     2              N           NaN   \n",
       "4                     8              N           NaN   \n",
       "\n",
       "  MEAL_REIMBURSEMENT_MINIMUM MEAL_REIMBURSEMENT_MAXIMUM PHONE_TO_APPLY  \\\n",
       "0                      12.68                       55.0    13607017661   \n",
       "1                      12.68                       55.0    19318083783   \n",
       "2                      12.68                       55.0    19369333827   \n",
       "3                      12.68                       55.0    14069637560   \n",
       "4                      12.68                       55.0    18632939888   \n",
       "\n",
       "                  EMAIL_TO_APPLY                             WEBSITE_TO_APPLY  \\\n",
       "0           faziofarms@gmail.com                                          NaN   \n",
       "1                            NaN  https://www.jobs4tn.gov/vosnet/Default.aspx   \n",
       "2  fayethlynpitre@rocketmail.com                                          NaN   \n",
       "3        lodahl_kelsey@yahoo.com                                          NaN   \n",
       "4                            NaN                        www.employflorida.com   \n",
       "\n",
       "   TOTAL_ADDENDUM_A_RECORDS  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         4  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here to load the data \n",
    "jobs = pd.read_csv('./pset2_inputdata/jobs.csv')\n",
    "\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Try inner join on employer name  (2 points)\n",
    "\n",
    "- Use the `EMPLOYER_NAME` field of the `jobs` dataset\n",
    "- Use the `Name` field of the `debar_clean` dataset \n",
    "\n",
    "A. Use pd.merge with an inner join on those fields to see whether there are any exact matches. \n",
    "\n",
    "B. If there are exact matches, print the row(s) with exact matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NUMBER</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>RECEIVED_DATE</th>\n",
       "      <th>DECISION_DATE</th>\n",
       "      <th>TYPE_OF_EMPLOYER_APPLICATION</th>\n",
       "      <th>H2A_LABOR_CONTRACTOR</th>\n",
       "      <th>NATURE_OF_TEMPORARY_NEED</th>\n",
       "      <th>EMERGENCY_FILING</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>TRADE_NAME_DBA</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTAL_ADDENDUM_A_RECORDS</th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "      <th>is_dup</th>\n",
       "      <th>violnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-300-20287-876656</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-10-20 09:20:32.010</td>\n",
       "      <td>2020-11-09 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>Y</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>Y</td>\n",
       "      <td>Rafael Barajas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rafael Barajas</td>\n",
       "      <td>Sebring, Florida</td>\n",
       "      <td>Non-payment of certification fee</td>\n",
       "      <td>1 year</td>\n",
       "      <td>9/23/2016</td>\n",
       "      <td>9/22/2017</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CASE_NUMBER                           CASE_STATUS  \\\n",
       "0  H-300-20287-876656  Determination Issued - Certification   \n",
       "\n",
       "             RECEIVED_DATE            DECISION_DATE  \\\n",
       "0  2020-10-20 09:20:32.010  2020-11-09 00:00:00.000   \n",
       "\n",
       "  TYPE_OF_EMPLOYER_APPLICATION H2A_LABOR_CONTRACTOR NATURE_OF_TEMPORARY_NEED  \\\n",
       "0          Individual Employer                    Y                 Seasonal   \n",
       "\n",
       "  EMERGENCY_FILING   EMPLOYER_NAME TRADE_NAME_DBA  ...  \\\n",
       "0                Y  Rafael Barajas            NaN  ...   \n",
       "\n",
       "  TOTAL_ADDENDUM_A_RECORDS            Name       City, State  \\\n",
       "0                        7  Rafael Barajas  Sebring, Florida   \n",
       "\n",
       "                          Violation Duration Start date   End date  \\\n",
       "0  Non-payment of certification fee   1 year  9/23/2016  9/22/2017   \n",
       "\n",
       "   is_repeated  is_dup  violnum  \n",
       "0        False     NaN        1  \n",
       "\n",
       "[1 rows x 147 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "\n",
    "merged = pd.merge(jobs, debar_clean,left_on='EMPLOYER_NAME', right_on='Name',how='inner')\n",
    "merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Targeted regex (10 points total)\n",
    "\n",
    "You want to see if you can increase the exact match rate with some basic cleaning of each \n",
    "of the employer name fields in each dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Converting to upper (2 points)\n",
    "\n",
    "A. Convert the `EMPLOYER_NAME` and `Name` fields to uppercase using list comprehension rather than df.varname.str.upper() (it's fine to do a separate list comprehension line for each of the two columns)\n",
    "\n",
    "B. Print a random sample of 15 values of each result\n",
    "\n",
    "C. Assign the full vector of uppercase names back to the original data, writing over the original `EMPLOYER_NAME` and `Name` columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code to turn into uppercase here\n",
    "jobs['EMPLOYER_NAME']=[name.upper() for name in jobs.EMPLOYER_NAME]\n",
    "debar_clean['NAME']=[name.upper() for name in debar_clean.Name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49                                      STODDARD RANCH \n",
       "934                             WALTHER FARMS, LLC - NE\n",
       "373                                          TREW JAMES\n",
       "576                           WESTERN RANGE ASSOCIATION\n",
       "2471    VIRGINIA AGRICULTURAL GROWERS ASSOCIATION, INC.\n",
       "769                           JESUS MARTINEZ SANTIBANEZ\n",
       "1301                              MICHAEL TODD MCDANIEL\n",
       "2340                   WALLA WALLA NURSERY COMPANY INC.\n",
       "657                                       Y CROSS, INC.\n",
       "1571                                      TREVOR B HAIR\n",
       "2057                                 HARTWELL FARM INC.\n",
       "846                                   BERGER CATTLE CO.\n",
       "1796      THE NORTH CAROLINA GROWER'S ASSOCIATION, INC.\n",
       "411                       CRISP CITRUS HARVESTING, LLC.\n",
       "940                                 BONNIE PLANTS, INC.\n",
       "Name: EMPLOYER_NAME, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "70                 John & Neta Leopky Farms\n",
       "20                     Slash E.V. Ranch LLP\n",
       "89                             Xavier Horne\n",
       "96                         Nemorio Resendiz\n",
       "12              Rollo Farm Labor Contractor\n",
       "37                             John R. Cook\n",
       "83                        Cira Cortez Lopez\n",
       "22                     Alteric Jean-Charles\n",
       "94                             69 Farms LLC\n",
       "18                                F&W Farms\n",
       "14                            Sharon Mathis\n",
       "87                        Leslie Renee Drew\n",
       "95                      Vern Stratton Farms\n",
       "8     Caddo Creek Ranch, dba Paradise Ranch\n",
       "73                             Xavier Horne\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## insert your code for the random sample\n",
    "jobs.EMPLOYER_NAME.sample(15)\n",
    "debar_clean.Name.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code for assigning the uppercase names back to the data\n",
    "\n",
    "#already assigned in part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Cleaning up punctuation (4 points)\n",
    "\n",
    "You notice that INC, CO, and LLC are sometimes followed by a period (.) but sometimes not\n",
    "\n",
    "A. For each dataset, write a regex pattern using `re.sub` to remove the . but only if it's preceded by INC, LLC, or CO \n",
    "\n",
    "Make sure LLC, INC, CO remain part of the string but just without the dot\n",
    "\n",
    "B. Test the pattern on the positive and negative example we provide below and print the result. See the Github issue for examples of what to return\n",
    "\n",
    "\n",
    "**Hint**: https://stackoverflow.com/questions/7191209/python-re-sub-replace-with-matched-content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_example_1 = \"CISCO PRODUCE INC.\"\n",
    "pos_example_2 = \"AVOYELLES HONEY CO., LLC\"\n",
    "neg_example = \"E.V. RANCH LLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CISCO PRODUCE INC'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'AVOYELLES HONEY CO, LLC'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'E.V. RANCH LLP'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## insert your code here with the regex pattern for part A\n",
    "pattern = r\"(\\s+)(INC|LLC|CO)\\.\"\n",
    "\n",
    "## insert your code to use re.sub to apply the pattern to the test cases for part B\n",
    "re.sub(pattern,r\"\\1\\2\",pos_example_1)\n",
    "re.sub(pattern,r\"\\1\\2\",pos_example_2)\n",
    "re.sub(pattern,r\"\\1\\2\",neg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 (4 points)\n",
    "\n",
    "Use that pattern in conjunction with `re.sub` and list comprehension to clean the employer name columns in each dataset. Save the new columns as `name_clean` in each. Then, use row subsetting to (1) subset to rows that changed names and (2) for:\n",
    "\n",
    "- `debar_clean` print the `Name` and `name_clean` columns\n",
    "- `jobs` print the `EMPLOYER_NAME` and `name_clean` columns\n",
    "\n",
    "Make sure to use the uppercase versions of the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>name_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANTON FERTILIZER INC.</td>\n",
       "      <td>ANTON FERTILIZER INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GREAT PLAINS FLUID SERVICE, INC.</td>\n",
       "      <td>GREAT PLAINS FLUID SERVICE, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PROMAX INC.</td>\n",
       "      <td>PROMAX INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>REIMER'S INC.</td>\n",
       "      <td>REIMER'S INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CISCO PRODUCE INC.</td>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>REIMER’S INC.</td>\n",
       "      <td>REIMER’S INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GERONIMO SHEEP CO.</td>\n",
       "      <td>GERONIMO SHEEP CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ALTENDORF TRANSPORT INC.</td>\n",
       "      <td>ALTENDORF TRANSPORT INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>CISCO PRODUCE INC.</td>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>SAXTONS RIVER ORCHARDS, INC.</td>\n",
       "      <td>SAXTONS RIVER ORCHARDS, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>AVOYELLES HONEY CO., LLC</td>\n",
       "      <td>AVOYELLES HONEY CO, LLC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                NAME                       name_clean\n",
       "3              ANTON FERTILIZER INC.             ANTON FERTILIZER INC\n",
       "4   GREAT PLAINS FLUID SERVICE, INC.  GREAT PLAINS FLUID SERVICE, INC\n",
       "5                        PROMAX INC.                       PROMAX INC\n",
       "13                     REIMER'S INC.                     REIMER'S INC\n",
       "19                CISCO PRODUCE INC.                CISCO PRODUCE INC\n",
       "23                     REIMER’S INC.                     REIMER’S INC\n",
       "33                GERONIMO SHEEP CO.                GERONIMO SHEEP CO\n",
       "46          ALTENDORF TRANSPORT INC.          ALTENDORF TRANSPORT INC\n",
       "56                CISCO PRODUCE INC.                CISCO PRODUCE INC\n",
       "72      SAXTONS RIVER ORCHARDS, INC.      SAXTONS RIVER ORCHARDS, INC\n",
       "91          AVOYELLES HONEY CO., LLC          AVOYELLES HONEY CO, LLC"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>name_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUNSON HARVESTING, INC.</td>\n",
       "      <td>DUNSON HARVESTING, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FARM LABOR ASSOCIATION FOR GROWERS, INC.</td>\n",
       "      <td>FARM LABOR ASSOCIATION FOR GROWERS, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MCLAIN FARMS, INC.</td>\n",
       "      <td>MCLAIN FARMS, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BONNIE PLANTS, INC.</td>\n",
       "      <td>BONNIE PLANTS, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B &amp; W QUALITY GROWERS, INC.</td>\n",
       "      <td>B &amp; W QUALITY GROWERS, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>HARRAL LIVESTOCK CO. LLC</td>\n",
       "      <td>HARRAL LIVESTOCK CO LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>ECOSYSTEM CONCEPTS INC.</td>\n",
       "      <td>ECOSYSTEM CONCEPTS INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>SIDDOWAY SHEEP CO.</td>\n",
       "      <td>SIDDOWAY SHEEP CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>SATHER MANAGEMENT INC.</td>\n",
       "      <td>SATHER MANAGEMENT INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>C R KOEHL AND SONS INC.</td>\n",
       "      <td>C R KOEHL AND SONS INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 EMPLOYER_NAME  \\\n",
       "4                      DUNSON HARVESTING, INC.   \n",
       "7     FARM LABOR ASSOCIATION FOR GROWERS, INC.   \n",
       "14                          MCLAIN FARMS, INC.   \n",
       "17                         BONNIE PLANTS, INC.   \n",
       "18                 B & W QUALITY GROWERS, INC.   \n",
       "...                                        ...   \n",
       "2700                  HARRAL LIVESTOCK CO. LLC   \n",
       "2701                   ECOSYSTEM CONCEPTS INC.   \n",
       "2702                        SIDDOWAY SHEEP CO.   \n",
       "2705                    SATHER MANAGEMENT INC.   \n",
       "2711                   C R KOEHL AND SONS INC.   \n",
       "\n",
       "                                   name_clean  \n",
       "4                      DUNSON HARVESTING, INC  \n",
       "7     FARM LABOR ASSOCIATION FOR GROWERS, INC  \n",
       "14                          MCLAIN FARMS, INC  \n",
       "17                         BONNIE PLANTS, INC  \n",
       "18                 B & W QUALITY GROWERS, INC  \n",
       "...                                       ...  \n",
       "2700                  HARRAL LIVESTOCK CO LLC  \n",
       "2701                   ECOSYSTEM CONCEPTS INC  \n",
       "2702                        SIDDOWAY SHEEP CO  \n",
       "2705                    SATHER MANAGEMENT INC  \n",
       "2711                   C R KOEHL AND SONS INC  \n",
       "\n",
       "[575 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here to clean the columns\n",
    "pattern = r\"(\\s+)(INC|LLC|CO)\\.\"\n",
    "\n",
    "debar_clean['name_clean'] = [re.sub(pattern,r\"\\1\\2\",name) for name in debar_clean.NAME] \n",
    "jobs['name_clean'] = [re.sub(pattern,r\"\\1\\2\",name) for name in jobs.EMPLOYER_NAME]\n",
    "\n",
    "\n",
    "debar_clean[debar_clean.NAME != debar_clean.name_clean][['NAME','name_clean']]\n",
    "jobs[jobs.EMPLOYER_NAME != jobs.name_clean][['EMPLOYER_NAME','name_clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "      <th>is_dup</th>\n",
       "      <th>violnum</th>\n",
       "      <th>NAME</th>\n",
       "      <th>name_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Annabella Land &amp; Cattle</td>\n",
       "      <td>Annabella, UT</td>\n",
       "      <td>Non Payment</td>\n",
       "      <td>1 year</td>\n",
       "      <td>5/9/2014</td>\n",
       "      <td>5/9/2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>ANNABELLA LAND &amp; CATTLE</td>\n",
       "      <td>ANNABELLA LAND &amp; CATTLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autumn Hill Orchard</td>\n",
       "      <td>Groton, MA</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/5/2016</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTUMN HILL ORCHARD</td>\n",
       "      <td>AUTUMN HILL ORCHARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Caddo Creek Ranch, dba Paradise Ranch</td>\n",
       "      <td>Caddo, TX</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>7/20/2014</td>\n",
       "      <td>7/19/2016</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>CADDO CREEK RANCH, DBA PARADISE RANCH</td>\n",
       "      <td>CADDO CREEK RANCH, DBA PARADISE RANCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Loewen Harvesting LLC</td>\n",
       "      <td>Brownsville, TX</td>\n",
       "      <td>Failure to respond to audit (partial response)</td>\n",
       "      <td>1 year</td>\n",
       "      <td>8/20/2014</td>\n",
       "      <td>8/19/2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>LOEWEN HARVESTING LLC</td>\n",
       "      <td>LOEWEN HARVESTING LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rollo Farm Labor Contractor</td>\n",
       "      <td>Miami, FL</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "      <td>2 years</td>\n",
       "      <td>8/23/2014</td>\n",
       "      <td>8/22/2016</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>ROLLO FARM LABOR CONTRACTOR</td>\n",
       "      <td>ROLLO FARM LABOR CONTRACTOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Name      City, State  \\\n",
       "6                 Annabella Land & Cattle    Annabella, UT   \n",
       "7                     Autumn Hill Orchard       Groton, MA   \n",
       "8   Caddo Creek Ranch, dba Paradise Ranch        Caddo, TX   \n",
       "11                  Loewen Harvesting LLC  Brownsville, TX   \n",
       "12            Rollo Farm Labor Contractor        Miami, FL   \n",
       "\n",
       "                                         Violation Duration Start date  \\\n",
       "6                                      Non Payment   1 year   5/9/2014   \n",
       "7        Failure to respond to audit (no response)  2 years   7/6/2014   \n",
       "8   Failure to respond to audit (partial response)  2 years  7/20/2014   \n",
       "11  Failure to respond to audit (partial response)   1 year  8/20/2014   \n",
       "12       Failure to respond to audit (no response)  2 years  8/23/2014   \n",
       "\n",
       "     End date  is_repeated is_dup  violnum  \\\n",
       "6    5/9/2015         True   True        1   \n",
       "7    7/5/2016         True   True        1   \n",
       "8   7/19/2016         True   True        1   \n",
       "11  8/19/2015         True   True        1   \n",
       "12  8/22/2016         True   True        1   \n",
       "\n",
       "                                     NAME  \\\n",
       "6                 ANNABELLA LAND & CATTLE   \n",
       "7                     AUTUMN HILL ORCHARD   \n",
       "8   CADDO CREEK RANCH, DBA PARADISE RANCH   \n",
       "11                  LOEWEN HARVESTING LLC   \n",
       "12            ROLLO FARM LABOR CONTRACTOR   \n",
       "\n",
       "                               name_clean  \n",
       "6                 ANNABELLA LAND & CATTLE  \n",
       "7                     AUTUMN HILL ORCHARD  \n",
       "8   CADDO CREEK RANCH, DBA PARADISE RANCH  \n",
       "11                  LOEWEN HARVESTING LLC  \n",
       "12            ROLLO FARM LABOR CONTRACTOR  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NUMBER</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>RECEIVED_DATE</th>\n",
       "      <th>DECISION_DATE</th>\n",
       "      <th>TYPE_OF_EMPLOYER_APPLICATION</th>\n",
       "      <th>H2A_LABOR_CONTRACTOR</th>\n",
       "      <th>NATURE_OF_TEMPORARY_NEED</th>\n",
       "      <th>EMERGENCY_FILING</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>TRADE_NAME_DBA</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTAL_HOUSING_RECORDS</th>\n",
       "      <th>MEALS_PROVIDED</th>\n",
       "      <th>MEALS_CHARGED</th>\n",
       "      <th>MEAL_REIMBURSEMENT_MINIMUM</th>\n",
       "      <th>MEAL_REIMBURSEMENT_MAXIMUM</th>\n",
       "      <th>PHONE_TO_APPLY</th>\n",
       "      <th>EMAIL_TO_APPLY</th>\n",
       "      <th>WEBSITE_TO_APPLY</th>\n",
       "      <th>TOTAL_ADDENDUM_A_RECORDS</th>\n",
       "      <th>name_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-300-20199-721302</td>\n",
       "      <td>Determination Issued - Withdrawn</td>\n",
       "      <td>2020-07-17 14:50:40.840</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>Y</td>\n",
       "      <td>FAZIO FARMS OPERATING COMPANY, LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>12.68</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>13607017661</td>\n",
       "      <td>faziofarms@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FAZIO FARMS OPERATING COMPANY, LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-300-20231-773906</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-20 10:38:15.620</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Association - Agent</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>CHARLIE SUNDERLAND</td>\n",
       "      <td>Panter &amp; Sunderland Nursery</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>19318083783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.jobs4tn.gov/vosnet/Default.aspx</td>\n",
       "      <td>0</td>\n",
       "      <td>CHARLIE SUNDERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-300-20231-774123</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-24 15:33:14.340</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>MICHAEL RUDEBUSCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>19369333827</td>\n",
       "      <td>fayethlynpitre@rocketmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>MICHAEL RUDEBUSCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-300-20231-774151</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-21 12:08:09.760</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>N</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>LODAHL FARMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14069637560</td>\n",
       "      <td>lodahl_kelsey@yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>LODAHL FARMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-300-20231-774508</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-08-20 10:17:34.530</td>\n",
       "      <td>2020-10-01 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>Y</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>N</td>\n",
       "      <td>DUNSON HARVESTING, INC.</td>\n",
       "      <td>Dunson Harvesting, Inc.</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18632939888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.employflorida.com</td>\n",
       "      <td>4</td>\n",
       "      <td>DUNSON HARVESTING, INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CASE_NUMBER                           CASE_STATUS  \\\n",
       "0  H-300-20199-721302      Determination Issued - Withdrawn   \n",
       "1  H-300-20231-773906  Determination Issued - Certification   \n",
       "2  H-300-20231-774123  Determination Issued - Certification   \n",
       "3  H-300-20231-774151  Determination Issued - Certification   \n",
       "4  H-300-20231-774508  Determination Issued - Certification   \n",
       "\n",
       "             RECEIVED_DATE            DECISION_DATE  \\\n",
       "0  2020-07-17 14:50:40.840  2020-10-01 00:00:00.000   \n",
       "1  2020-08-20 10:38:15.620  2020-10-01 00:00:00.000   \n",
       "2  2020-08-24 15:33:14.340  2020-10-01 00:00:00.000   \n",
       "3  2020-08-21 12:08:09.760  2020-10-01 00:00:00.000   \n",
       "4  2020-08-20 10:17:34.530  2020-10-01 00:00:00.000   \n",
       "\n",
       "  TYPE_OF_EMPLOYER_APPLICATION H2A_LABOR_CONTRACTOR NATURE_OF_TEMPORARY_NEED  \\\n",
       "0          Individual Employer                    N                 Seasonal   \n",
       "1          Association - Agent                    N                 Seasonal   \n",
       "2          Individual Employer                    N                 Seasonal   \n",
       "3          Individual Employer                    N                 Seasonal   \n",
       "4          Individual Employer                    Y                 Seasonal   \n",
       "\n",
       "  EMERGENCY_FILING                       EMPLOYER_NAME  \\\n",
       "0                Y  FAZIO FARMS OPERATING COMPANY, LLC   \n",
       "1                N                  CHARLIE SUNDERLAND   \n",
       "2                N                   MICHAEL RUDEBUSCH   \n",
       "3                N                        LODAHL FARMS   \n",
       "4                N             DUNSON HARVESTING, INC.   \n",
       "\n",
       "                TRADE_NAME_DBA  ... TOTAL_HOUSING_RECORDS MEALS_PROVIDED  \\\n",
       "0                          NaN  ...                     1              Y   \n",
       "1  Panter & Sunderland Nursery  ...                     1              N   \n",
       "2                          NaN  ...                     1              N   \n",
       "3                          NaN  ...                     2              N   \n",
       "4      Dunson Harvesting, Inc.  ...                     8              N   \n",
       "\n",
       "  MEALS_CHARGED MEAL_REIMBURSEMENT_MINIMUM MEAL_REIMBURSEMENT_MAXIMUM  \\\n",
       "0         12.68                      12.68                       55.0   \n",
       "1           NaN                      12.68                       55.0   \n",
       "2           NaN                      12.68                       55.0   \n",
       "3           NaN                      12.68                       55.0   \n",
       "4           NaN                      12.68                       55.0   \n",
       "\n",
       "  PHONE_TO_APPLY                 EMAIL_TO_APPLY  \\\n",
       "0    13607017661           faziofarms@gmail.com   \n",
       "1    19318083783                            NaN   \n",
       "2    19369333827  fayethlynpitre@rocketmail.com   \n",
       "3    14069637560        lodahl_kelsey@yahoo.com   \n",
       "4    18632939888                            NaN   \n",
       "\n",
       "                              WEBSITE_TO_APPLY  TOTAL_ADDENDUM_A_RECORDS  \\\n",
       "0                                          NaN                         0   \n",
       "1  https://www.jobs4tn.gov/vosnet/Default.aspx                         0   \n",
       "2                                          NaN                         0   \n",
       "3                                          NaN                         0   \n",
       "4                        www.employflorida.com                         4   \n",
       "\n",
       "                           name_clean  \n",
       "0  FAZIO FARMS OPERATING COMPANY, LLC  \n",
       "1                  CHARLIE SUNDERLAND  \n",
       "2                   MICHAEL RUDEBUSCH  \n",
       "3                        LODAHL FARMS  \n",
       "4              DUNSON HARVESTING, INC  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here to print the head\n",
    "debar_clean.head()\n",
    "jobs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 More joins and more cleaning (5 points)\n",
    "\n",
    "A. Conduct another inner join between `jobs` and `debar_clean` now using the `name_clean` column; print the result. Did the cleaning result in any more employers matched between the two datasets?\n",
    "\n",
    "B. Create a new column in `debar_clean` called `name_clean_2` that uses regex to take the following name in that dataset:\n",
    "\n",
    "- `SLASH E.V. RANCH LLP` in the `debar_clean` dataset\n",
    "\n",
    "And cleans it up so that it matches with this employer in `jobs`\n",
    "\n",
    "- `SLASH EV RANCH` in the `jobs` dataset\n",
    "\n",
    "Eg a pattern to remove the dots in the EV and the space+LLP-- you can apply the pattern to all employer names in debar_clean (so don't need to worry about only applying it to that one employer)\n",
    "\n",
    "\n",
    "C. Conduct a left join using `name_clean_2` as the join column where the left hand dataframe is `jobs`; right hand dataframe is `debar_clean`, store the result as a dataframe, and print the rows where the merge indicator indicates the row was found in both dataframe\n",
    "\n",
    "**Note**: this manual cleaning process is inefficient and helps motivate why talked about fuzzy matching. Fuzzy matching could recognize that Slash EV ranch is a highly similar string to slash ev ranch llp and match them without us needing to use regex to make the strings identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City, State</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "      <th>is_dup</th>\n",
       "      <th>violnum</th>\n",
       "      <th>NAME</th>\n",
       "      <th>name_clean</th>\n",
       "      <th>name_clean_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Slash E.V. Ranch LLP</td>\n",
       "      <td>Rifle, CO</td>\n",
       "      <td>WHD Debarment</td>\n",
       "      <td>1 year</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>11/14/2015</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SLASH E.V. RANCH LLP</td>\n",
       "      <td>SLASH E.V. RANCH LLP</td>\n",
       "      <td>SLASH EV RANCH LLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>E.V. Ranch LLP</td>\n",
       "      <td>Rifle, CO</td>\n",
       "      <td>Wage Hour Debarment</td>\n",
       "      <td>1 year</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>11/14/2015</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>E.V. RANCH LLP</td>\n",
       "      <td>E.V. RANCH LLP</td>\n",
       "      <td>EV RANCH LLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name City, State            Violation Duration  \\\n",
       "20  Slash E.V. Ranch LLP   Rifle, CO        WHD Debarment   1 year   \n",
       "50        E.V. Ranch LLP   Rifle, CO  Wage Hour Debarment   1 year   \n",
       "\n",
       "    Start date    End date  is_repeated is_dup  violnum                  NAME  \\\n",
       "20  11/15/2014  11/14/2015        False    NaN        1  SLASH E.V. RANCH LLP   \n",
       "50  11/15/2014  11/14/2015        False    NaN        1        E.V. RANCH LLP   \n",
       "\n",
       "              name_clean        name_clean_2  \n",
       "20  SLASH E.V. RANCH LLP  SLASH EV RANCH LLP  \n",
       "50        E.V. RANCH LLP        EV RANCH LLP  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NUMBER</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>RECEIVED_DATE</th>\n",
       "      <th>DECISION_DATE</th>\n",
       "      <th>TYPE_OF_EMPLOYER_APPLICATION</th>\n",
       "      <th>H2A_LABOR_CONTRACTOR</th>\n",
       "      <th>NATURE_OF_TEMPORARY_NEED</th>\n",
       "      <th>EMERGENCY_FILING</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>TRADE_NAME_DBA</th>\n",
       "      <th>...</th>\n",
       "      <th>Violation</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>is_repeated</th>\n",
       "      <th>is_dup</th>\n",
       "      <th>violnum</th>\n",
       "      <th>NAME</th>\n",
       "      <th>name_clean_y</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>H-300-20287-876656</td>\n",
       "      <td>Determination Issued - Certification</td>\n",
       "      <td>2020-10-20 09:20:32.010</td>\n",
       "      <td>2020-11-09 00:00:00.000</td>\n",
       "      <td>Individual Employer</td>\n",
       "      <td>Y</td>\n",
       "      <td>Seasonal</td>\n",
       "      <td>Y</td>\n",
       "      <td>RAFAEL BARAJAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-payment of certification fee</td>\n",
       "      <td>1 year</td>\n",
       "      <td>9/23/2016</td>\n",
       "      <td>9/22/2017</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RAFAEL BARAJAS</td>\n",
       "      <td>RAFAEL BARAJAS</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CASE_NUMBER                           CASE_STATUS  \\\n",
       "791  H-300-20287-876656  Determination Issued - Certification   \n",
       "\n",
       "               RECEIVED_DATE            DECISION_DATE  \\\n",
       "791  2020-10-20 09:20:32.010  2020-11-09 00:00:00.000   \n",
       "\n",
       "    TYPE_OF_EMPLOYER_APPLICATION H2A_LABOR_CONTRACTOR  \\\n",
       "791          Individual Employer                    Y   \n",
       "\n",
       "    NATURE_OF_TEMPORARY_NEED EMERGENCY_FILING   EMPLOYER_NAME TRADE_NAME_DBA  \\\n",
       "791                 Seasonal                Y  RAFAEL BARAJAS            NaN   \n",
       "\n",
       "     ...                         Violation Duration Start date   End date  \\\n",
       "791  ...  Non-payment of certification fee   1 year  9/23/2016  9/22/2017   \n",
       "\n",
       "    is_repeated is_dup violnum            NAME    name_clean_y  _merge  \n",
       "791       False    NaN     1.0  RAFAEL BARAJAS  RAFAEL BARAJAS    both  \n",
       "\n",
       "[1 rows x 152 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here\n",
    "merged = jobs.merge(debar_clean,on='name_clean',how='inner')\n",
    "\n",
    "pattern = r\"(.*)\\.(.*)\\.\"\n",
    "\n",
    "debar_clean['name_clean_2'] = [re.sub(pattern, r\"\\1\\2\", name) for name in debar_clean['name_clean']]\n",
    "jobs['name_clean_2'] = [re.sub(pattern, r\"\\1\\2\", name) for name in jobs['name_clean']]\n",
    "\n",
    "debar_clean[debar_clean['name_clean_2'] != debar_clean['name_clean'] ]\n",
    "\n",
    "merged_2 = jobs.merge(debar_clean,left_on=\"name_clean_2\",right_on=\"name_clean_2\",how='left',indicator=True)\n",
    "matched_rows = merged_2[merged_2['_merge'] == 'both']\n",
    "matched_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optional extra credit 1: regex to separate companies from individuals (1 point)\n",
    "\n",
    "You notice some employers in `debar_clean` have both the name of the company and the name of individual, e.g.:\n",
    "    \n",
    "COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMSON (INDIVIDUAL)*\n",
    "\n",
    "Use the uppercase/cleaned `name_clean` in `debar_clean`\n",
    "\n",
    "A. Write a regex pattern that does the following:\n",
    "    - Captures the pattern that occurs before COMPANY if (COMPANY) is in string; so in example above, extracts COUNTY FAIR FARM \n",
    "    - Captures the pattern that occurs before INDIVIDUAL if (INDIVIDUAL) is also in string -- so in above, extracts ANDREW WILLIAMSON (so omit the \"and\")\n",
    "    \n",
    "B. Test the pattern on `pos_example` and `neg_example`-- make sure former returns a list (if using find.all) or match object (if using re.search) with the company name and individual name separated out; make sure latter returns empty\n",
    "    \n",
    "**Hints and resources**: for step A, you can either use re.search, re.match, or re.findall; don't worry about matching B&R Harvesting and Paul Cruz (Individual)\n",
    "\n",
    "- Same regex resources as above\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('COUNTY FAIR FARM', 'ANDREW WILLIAMSON'), [])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_example = \"COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMSON (INDIVIDUAL)*\"\n",
    "neg_example = \"CISCO PRODUCE INC\"\n",
    "\n",
    "## your code here to define the pattern\n",
    "import re\n",
    "\n",
    "pattern = r\"^(.*?)\\s+\\(COMPANY\\)\\s+AND\\s+(.*?)\\s+\\(INDIVIDUAL\\)\"\n",
    "\n",
    "## your code here to apply it to the pos_example\n",
    "match_pos = re.search(pattern, pos_example)\n",
    "result_pos = match_pos.groups() if match_pos else []\n",
    "\n",
    "## your code here to apply it to the negative example\n",
    "match_neg = re.search(pattern, neg_example)\n",
    "result_neg = match_neg.groups() if match_neg else []\n",
    "\n",
    "## Results\n",
    "result_pos, result_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Iterate over the `name_clean` column in debar and use regex to create two new columns in `debar_clean`:\n",
    "   - `co_name`: A column for company (full `name_clean` string if no match; pattern before COMPANY if one extracted)\n",
    "   - `ind_name`: A column for individual (full `name_clean` string if no match; pattern before INDIVIDUAL if one extracted)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_clean</th>\n",
       "      <th>co_name</th>\n",
       "      <th>ind_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMS...</td>\n",
       "      <td>COUNTY FAIR FARM</td>\n",
       "      <td>ANDREW WILLIAMSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name_clean           co_name  \\\n",
       "108  COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMS...  COUNTY FAIR FARM   \n",
       "\n",
       "              ind_name  \n",
       "108  ANDREW WILLIAMSON  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "pattern = r\"^(.*?)\\s+\\(COMPANY\\)\\s+AND\\s+(.*?)\\s+\\(INDIVIDUAL\\)\"\n",
    "\n",
    "# Define extraction function\n",
    "def extract_names(name):\n",
    "    match = re.search(pattern, name)\n",
    "    if match:\n",
    "        return pd.Series([match.group(1), match.group(2)])\n",
    "    else:\n",
    "        return pd.Series([name, name])\n",
    "\n",
    "# Apply to debar_clean\n",
    "debar_clean[['co_name', 'ind_name']] = debar_clean['name_clean'].apply(extract_names)\n",
    "\n",
    "# Filter for rows where company and individual were successfully split\n",
    "both_extracted = debar_clean[debar_clean['co_name'] != debar_clean['ind_name']]\n",
    "\n",
    "# Display result\n",
    "both_extracted[['name_clean', 'co_name', 'ind_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "D. Print three columns for the rows in `debar_clean` containing the negative example and positive example described above (county fair farm and cisco produce):\n",
    "\n",
    "- `name_clean`\n",
    "- `co_name`\n",
    "- `ind_name`\n",
    "- `Violation`\n",
    "\n",
    "**Note**: as shown in the outcome there may be duplicates of the same company reflecting different violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_clean</th>\n",
       "      <th>co_name</th>\n",
       "      <th>ind_name</th>\n",
       "      <th>Violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "      <td>Failure to respond to audit (no response)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "      <td>CISCO PRODUCE INC</td>\n",
       "      <td>Impeding the Audit Process – Non- Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMS...</td>\n",
       "      <td>COUNTY FAIR FARM</td>\n",
       "      <td>ANDREW WILLIAMSON</td>\n",
       "      <td>WHD Debarment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name_clean            co_name  \\\n",
       "19                                   CISCO PRODUCE INC  CISCO PRODUCE INC   \n",
       "56                                   CISCO PRODUCE INC  CISCO PRODUCE INC   \n",
       "108  COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMS...   COUNTY FAIR FARM   \n",
       "\n",
       "              ind_name                                   Violation  \n",
       "19   CISCO PRODUCE INC   Failure to respond to audit (no response)  \n",
       "56   CISCO PRODUCE INC  Impeding the Audit Process – Non- Response  \n",
       "108  ANDREW WILLIAMSON                               WHD Debarment  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "examples = ['COUNTY FAIR FARM', 'CISCO PRODUCE']\n",
    "\n",
    "matches = debar_clean[debar_clean['name_clean'].str.contains('|'.join(examples), case=False)]\n",
    "\n",
    "# Display relevant columns\n",
    "matches[['name_clean', 'co_name', 'ind_name', 'Violation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optional extra credit 2 (up to 3 points)\n",
    "\n",
    "- For 1 point extra credit, create a visualization with 1+ of the existing fields in either the raw `jobs` or `debar` data. We'll be showing cool visualizations in class so use your imagination! Options could include visualizing between-state or over-time variation\n",
    "\n",
    "- For 3 points extra credit instead, geocode the employer addresses in `jobs` and plot the addresses of jobs as points overlaid on top of a map of Georgia \n",
    "    - **Note**: this extra credit involves Googling since we have not yet covered spatial data. \n",
    "        - For discussion of how to geocode addresses -> lat/long, see: https://www.natasshaselvaraj.com/a-step-by-step-guide-on-geocoding-in-python/ \n",
    "        - For discussion of plotting lat/long dots against a map, see this discussion of geopandas: https://towardsdatascience.com/plotting-maps-with-geopandas-428c97295a73\n",
    "    - Relevant columns include `EMPLOYER_ADDRESS_1` \n",
    "    - The geocoding might have a long runtime so feel free to implement it in a separate .py script that you submit alongside your notebook and to just read in the geocoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Users/trak/anaconda3/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: geopandas in /Users/trak/anaconda3/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/trak/anaconda3/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: contextily in /Users/trak/anaconda3/lib/python3.11/site-packages (1.6.2)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopandas) (0.9.0)\n",
      "Requirement already satisfied: packaging in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopandas) (23.1)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /Users/trak/anaconda3/lib/python3.11/site-packages (from geopandas) (2.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/trak/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: mercantile in /Users/trak/anaconda3/lib/python3.11/site-packages (from contextily) (1.2.1)\n",
      "Requirement already satisfied: rasterio in /Users/trak/anaconda3/lib/python3.11/site-packages (from contextily) (1.3.10)\n",
      "Requirement already satisfied: requests in /Users/trak/anaconda3/lib/python3.11/site-packages (from contextily) (2.31.0)\n",
      "Requirement already satisfied: joblib in /Users/trak/anaconda3/lib/python3.11/site-packages (from contextily) (1.2.0)\n",
      "Requirement already satisfied: xyzservices in /Users/trak/anaconda3/lib/python3.11/site-packages (from contextily) (2022.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/trak/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/trak/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: certifi in /Users/trak/anaconda3/lib/python3.11/site-packages (from pyogrio>=0.7.2->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/trak/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: click>=3.0 in /Users/trak/anaconda3/lib/python3.11/site-packages (from mercantile->contextily) (8.0.4)\n",
      "Requirement already satisfied: affine in /Users/trak/anaconda3/lib/python3.11/site-packages (from rasterio->contextily) (2.4.0)\n",
      "Requirement already satisfied: attrs in /Users/trak/anaconda3/lib/python3.11/site-packages (from rasterio->contextily) (23.2.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/trak/anaconda3/lib/python3.11/site-packages (from rasterio->contextily) (0.7.2)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /Users/trak/anaconda3/lib/python3.11/site-packages (from rasterio->contextily) (1.4.7)\n",
      "Requirement already satisfied: click-plugins in /Users/trak/anaconda3/lib/python3.11/site-packages (from rasterio->contextily) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/trak/anaconda3/lib/python3.11/site-packages (from rasterio->contextily) (68.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/trak/anaconda3/lib/python3.11/site-packages (from requests->contextily) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/trak/anaconda3/lib/python3.11/site-packages (from requests->contextily) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/trak/anaconda3/lib/python3.11/site-packages (from requests->contextily) (1.26.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy geopandas matplotlib contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/3 tries). Called with (*('400 Eagle Lake Loop Rd., Georgia, USA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 1378, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1311, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1167, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 357, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=400+Eagle+Lake+Loop+Rd.%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=400+Eagle+Lake+Loop+Rd.%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=400+Eagle+Lake+Loop+Rd.%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/3 tries). Called with (*('400 Eagle Lake Loop Rd., Georgia, USA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 1378, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1311, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1167, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 357, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=400+Eagle+Lake+Loop+Rd.%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=400+Eagle+Lake+Loop+Rd.%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=400+Eagle+Lake+Loop+Rd.%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (0/3 tries). Called with (*('101 E. Main Street, Georgia, USA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 1378, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1311, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1167, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 357, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/3 tries). Called with (*('101 E. Main Street, Georgia, USA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 1378, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1311, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1167, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 357, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (2/3 tries). Called with (*('101 E. Main Street, Georgia, USA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 1378, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1311, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/ssl.py\", line 1167, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 357, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 826, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/trak/anaconda3/lib/python3.11/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    },
    {
     "ename": "GeocoderUnavailable",
     "evalue": "HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:357\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:826\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    823\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    825\u001b[0m     )\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    827\u001b[0m         method,\n\u001b[1;32m    828\u001b[0m         url,\n\u001b[1;32m    829\u001b[0m         body,\n\u001b[1;32m    830\u001b[0m         headers,\n\u001b[1;32m    831\u001b[0m         retries,\n\u001b[1;32m    832\u001b[0m         redirect,\n\u001b[1;32m    833\u001b[0m         assert_same_host,\n\u001b[1;32m    834\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    835\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    836\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    837\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    838\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    840\u001b[0m     )\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:826\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    823\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    825\u001b[0m     )\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    827\u001b[0m         method,\n\u001b[1;32m    828\u001b[0m         url,\n\u001b[1;32m    829\u001b[0m         body,\n\u001b[1;32m    830\u001b[0m         headers,\n\u001b[1;32m    831\u001b[0m         retries,\n\u001b[1;32m    832\u001b[0m         redirect,\n\u001b[1;32m    833\u001b[0m         assert_same_host,\n\u001b[1;32m    834\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    835\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    836\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    837\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    838\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    840\u001b[0m     )\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/adapters.py:482\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGeocoderUnavailable\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m jobs_subset \u001b[38;5;241m=\u001b[39m jobs\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     10\u001b[0m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_address\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMPLOYER_ADDRESS_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Georgia, USA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_address\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(geocode)\n\u001b[1;32m     12\u001b[0m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m loc: loc\u001b[38;5;241m.\u001b[39mlatitude \u001b[38;5;28;01mif\u001b[39;00m loc \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m jobs_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m loc: loc\u001b[38;5;241m.\u001b[39mlongitude \u001b[38;5;28;01mif\u001b[39;00m loc \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py:284\u001b[0m, in \u001b[0;36mRateLimiter.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gen\u001b[38;5;241m.\u001b[39mthrow(e):\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;66;03m# A final try\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exc(args, kwargs)\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_wait_seconds)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not have been reached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py:136\u001b[0m, in \u001b[0;36mBaseRateLimiter._retries_gen\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, is_last_try \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(count(), _is_last_gen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries)):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i  \u001b[38;5;66;03m# Run the function.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_exceptions:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_last_try:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/extra/rate_limiter.py:274\u001b[0m, in \u001b[0;36mRateLimiter.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_request_slot()\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(res):\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn async awaitable has been passed to `RateLimiter`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `AsyncRateLimiter` instead, which supports awaitables.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/geocoders/nominatim.py:297\u001b[0m, in \u001b[0;36mNominatim.geocode\u001b[0;34m(self, query, exactly_one, timeout, limit, addressdetails, language, geometry, extratags, country_codes, viewbox, bounded, featuretype, namedetails)\u001b[0m\n\u001b[1;32m    295\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.geocode: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, url)\n\u001b[1;32m    296\u001b[0m callback \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_json, exactly_one\u001b[38;5;241m=\u001b[39mexactly_one)\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_geocoder(url, callback, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/geocoders/base.py:368\u001b[0m, in \u001b[0;36mGeocoder._call_geocoder\u001b[0;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_json:\n\u001b[0;32m--> 368\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter\u001b[38;5;241m.\u001b[39mget_json(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mreq_headers)\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter\u001b[38;5;241m.\u001b[39mget_text(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mreq_headers)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/adapters.py:472\u001b[0m, in \u001b[0;36mRequestsAdapter.get_json\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m, timeout, headers):\n\u001b[0;32m--> 472\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(url, timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/geopy/adapters.py:494\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GeocoderServiceError(message)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GeocoderUnavailable(message)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, requests\u001b[38;5;241m.\u001b[39mTimeout):\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GeocoderTimedOut(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mGeocoderUnavailable\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=101+E.+Main+Street%2C+Georgia%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))"
     ]
    }
   ],
   "source": [
    "## your code here\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "\n",
    "geolocator = Nominatim(\n",
    "    user_agent=\"dartmouth-pset2/1.0 (krittin.hirunchupong@dartmouth.edu)\",\n",
    "    timeout=10  # <-- Increase timeout here!\n",
    ")\n",
    "geocode = RateLimiter(\n",
    "    geolocator.geocode,\n",
    "    min_delay_seconds=1.5,         # Be nice to the server\n",
    "    max_retries=5,                 # More retries\n",
    "    error_wait_seconds=10,        # Wait longer before retrying\n",
    "    swallow_exceptions=True       # Don't crash on individual failure\n",
    ")\n",
    "\n",
    "jobs_subset = jobs.head(10).copy()\n",
    "\n",
    "jobs_subset['full_address'] = jobs_subset['EMPLOYER_ADDRESS_1'] + ', Georgia, USA'\n",
    "jobs_subset['location'] = jobs_subset['full_address'].apply(geocode)\n",
    "jobs_subset['latitude'] = jobs_subset['location'].apply(lambda loc: loc.latitude if loc else None)\n",
    "jobs_subset['longitude'] = jobs_subset['location'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "jobs_subset.to_csv(\"geocoded_jobs_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop rows with missing coordinates\n",
    "geo_jobs = jobs_subset.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    geo_jobs,\n",
    "    geometry=gpd.points_from_xy(geo_jobs.longitude, geo_jobs.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Plot\n",
    "gdf.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\n",
    "plt.title(\"Employer Job Locations in Georgia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
